{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63b8542-63a0-4421-835b-c427befd8799",
   "metadata": {},
   "source": [
    "# Search Engine: RAG Pipeline with Semantic Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d800c23-94a1-486a-81b0-bdda640ed216",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Retrieval-Augmented Generation (RAG)** - How to combine document retrieval with language models\n",
    "2. **Semantic Caching** - How to cache responses based on meaning, not exact matches\n",
    "3. **Vector Databases** - How embeddings enable semantic search\n",
    "4. **Query Routing** - How to intelligently route queries to appropriate data sources\n",
    "5. **Performance Optimization** - How caching improves response times and reduces costs\n",
    "\n",
    "## üìö What We'll Build\n",
    "\n",
    "A complete semantic search engine that:\n",
    "- Searches through 10-K financial documents and OpenAI documentation\n",
    "- Uses intelligent routing to determine the best data source\n",
    "- Implements semantic caching for 10x faster responses\n",
    "- Falls back to web search when needed\n",
    "- Provides detailed logging for learning and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da4bb8-978b-466e-8f44-8fd27b485253",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. üß† Understanding the Core Concepts\n",
    "\n",
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG combines the power of large language models with external knowledge retrieval:\n",
    "\n",
    "1. **Query** ‚Üí User asks a question\n",
    "2. **Retrieve** ‚Üí Find relevant documents using semantic search\n",
    "3. **Augment** ‚Üí Add retrieved context to the original query\n",
    "4. **Generate** ‚Üí LLM generates answer using both query and context\n",
    "\n",
    "**Why RAG?**\n",
    "- ‚úÖ Up-to-date information (not limited by training cutoff)\n",
    "- ‚úÖ Domain-specific knowledge\n",
    "- ‚úÖ Traceable sources\n",
    "- ‚úÖ Reduces hallucinations\n",
    "\n",
    "### Semantic Caching\n",
    "\n",
    "Traditional caching uses exact matches, but semantic caching understands meaning:\n",
    "\n",
    "- **Traditional**: \"What is Python?\" ‚â† \"Tell me about Python\"\n",
    "- **Semantic**: Both queries have similar meaning and can share cached results\n",
    "\n",
    "**How it works:**\n",
    "1. Convert queries to embeddings (vectors)\n",
    "2. Use similarity search to find semantically similar past queries\n",
    "3. Return cached response if similarity > threshold\n",
    "4. Otherwise, process query normally and cache result\n",
    "\n",
    "**Benefits:**\n",
    "- ‚ö° 10x faster responses (0.1s vs 1-3s)\n",
    "- üí∞ Reduced API costs\n",
    "- üéØ Handles paraphrasing and similar questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a08f8-b047-421d-ba02-8485fb22b977",
   "metadata": {},
   "source": [
    "## 2. üõ†Ô∏è Environment Setup\n",
    "\n",
    "Let's start by installing dependencies and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d04bbf5-5dbc-405a-b858-4470799433ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages. Uncomment if you have not installed\n",
    "#!pip install openai qdrant-client faiss-cpu  requests python-dotenv numpy pandas matplotlib seaborn tqdm ipywidgets python-dotenv\n",
    "\n",
    "# For Nomic embeddings\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c840ee-4c95-4485-bb58-1a25ebec4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple, functools, Callable\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# ML and embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# APIs\n",
    "from openai import OpenAI,OpenAIError\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import requests\n",
    "\n",
    "#ENV Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Embedding models (used for text vectorization during retrieval)\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebb968-56e7-47b1-b6a2-1417849c4186",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. üìÑ Sources\n",
    "\n",
    "For this demo we will set up the two main external sources of data:\n",
    "- Web Search: For external queries (e.g., \"latest Nvidia earnings\") using ARES.\n",
    "- Local Search from Qdrant Vector database\n",
    "  1. Vectorized OpenAI documentation\n",
    "  2. Vectorized 10-K financial filings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d499632-cedc-4064-ab09-05243b409252",
   "metadata": {},
   "source": [
    "### Web Search\n",
    "Let's set up Ares. The get_internet_content function will call ARES using an user query. Most of the code is from a previous notebook so look at RAG in [module 1](https://github.com/hamzafarooq/multi-agent-course/blob/main/Module_1/Agentic_RAG/Agentic_RAG_Notebook.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe0f94c-6440-4b07-85a9-f733f212d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ares_api_key=os.getenv('ARES_API_KEY')\n",
    "\n",
    "import requests  # For sending HTTP POST requests to the ARES API\n",
    "\n",
    "def get_internet_content(user_query: str, action: str):\n",
    "    \"\"\"\n",
    "    Fetches a response from the internet using ARES-API based on the user's query.\n",
    "\n",
    "    This function serves as the tool invoked when the router classifies a query\n",
    "    as requiring real-time information beyond internal datasets‚Äîi.e., \"INTERNET_QUERY\".\n",
    "    It sends the query to a live search API (ARES) and returns the result.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question that needs a live answer.\n",
    "        action (str): Route type (always expected to be \"INTERNET_QUERY\").\n",
    "\n",
    "    Returns:\n",
    "        str: Response text generated using internet search or an error message.\n",
    "    \"\"\"\n",
    "    print(\"Getting your response from the internet üåê ...\")\n",
    "\n",
    "    # API endpoint for the ARES live search tool\n",
    "    url = \"https://api-ares.traversaal.ai/live/predict\"\n",
    "\n",
    "    # Payload structure expected by the ARES API\n",
    "    payload = {\"query\": [user_query]}\n",
    "\n",
    "    # Authentication and content headers for API access\n",
    "    headers = {\n",
    "        \"x-api-key\": ares_api_key,  # Your secret API key (should be securely loaded from environment)\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send the query to the ARES API and check for success\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract and return the main response text from the API's nested JSON\n",
    "        return response.json().get('data', {}).get('response_text', \"No response received.\")\n",
    "\n",
    "    # Handle HTTP-level errors (e.g., 400s or 500s)\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        return f\"HTTP error occurred: {http_err}\"\n",
    "\n",
    "    # Handle general connection, timeout, or request formatting issues\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        return f\"Request error occurred: {req_err}\"\n",
    "\n",
    "    # Catch-all for any unexpected failure\n",
    "    except Exception as err:\n",
    "        return f\"An unexpected error occurred: {err}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb39a5-a3c1-4613-aea1-b1d22ed1c0f6",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "Here we will put two types of data 10-K financial filings and OpenAI documentation. We will take advantage of the metadata field so we can query based on the two types: \"10K_DOCUMENT_QUERY\" and \"OPENAI_QUERY\". We will need to setup the Qdrant vector database and incorporate the embeddings of individual documents. What are embeddings?\n",
    "\n",
    "**Embeddings** are vector representations of text that capture semantic meaning. Documents with similar meanings will have similar embeddings.\n",
    "\n",
    "### How Embeddings Enable Semantic Search:\n",
    "\n",
    "1. **Convert text to numbers**: \"Python programming\" ‚Üí [0.1, -0.3, 0.7, ...]\n",
    "2. **Measure similarity**: Use cosine similarity or euclidean distance\n",
    "3. **Find relevant content**: Similar embeddings = similar meaning\n",
    "\n",
    "### Why Vector Databases?\n",
    "\n",
    "- **Speed**: Optimized for similarity search across millions of vectors\n",
    "- **Scale**: Handle large document collections efficiently\n",
    "- **Flexibility**: Support metadata filtering and hybrid search\n",
    "\n",
    "The documents we will use will be from this github repository https://github.com/hamzafarooq/multi-agent-course.git. Let's create a Qdrant client and set the path to the repository and it should load the data that was already been converted to embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cb50c-47da-4768-93f0-6344bb1de2b4",
   "metadata": {},
   "source": [
    "## 4. üíæ Semantic Cache Implementation\n",
    "\n",
    "Now let's implement our semantic cache using FAISS for lightning-fast similarity search.\n",
    "\n",
    "### How Semantic Caching Works:\n",
    "\n",
    "1. **Store**: Query ‚Üí Embedding ‚Üí Cache with response\n",
    "2. **Lookup**: New query ‚Üí Embedding ‚Üí Find similar cached query\n",
    "3. **Match**: If similarity > threshold ‚Üí Return cached response\n",
    "4. **Miss**: Otherwise ‚Üí Process query normally ‚Üí Cache result\n",
    "\n",
    "### Performance Benefits:\n",
    "- **Speed**: 0.1s cache hit vs 1-3s API call\n",
    "- **Cost**: No API charges for cached responses  \n",
    "- **Intelligence**: Handles paraphrasing and similar questions\n",
    "\n",
    "Let's create a caching class that stores cached results in both in FAISS as well as in a file. A file is used when one wants to save a session with cached results as well as load previous cached results at the last session. The code below is alteration of the code that was shown in previous notebook in [semantic cache notebook](https://github.com/hamzafarooq/multi-agent-course/blob/main/Module_3/Semantic_Cache/Semantic_cache_from_scratch.ipynb) to adapt to other sources. The add method allows different sourcing functions for caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dde02bb-ec6f-4f0e-b528-d7640f305981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/hamzafarooq/multi-agent-course.git\n",
    "quadrant_client = QdrantClient(path=\"multi-agent-course/Module_1/Agentic_RAG/qdrant_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1737591-01c1-49b9-b440-55f08a7f0531",
   "metadata": {},
   "source": [
    "The get_text_embeddings function below coverts the text into an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66797081-8578-4666-ac93-5fb3ae7c7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "text_tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "text_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "def get_text_embeddings(text):\n",
    "    \"\"\"\n",
    "    Converts input text into a dense embedding using the Nomic embedding model.\n",
    "    These embeddings are used to query Qdrant for semantically relevant document chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text or query from the user.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A fixed-size vector representing the semantic meaning of the input.\n",
    "    \"\"\"\n",
    "    # Tokenize and prepare input for the model\n",
    "    inputs = text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Forward pass to get model outputs\n",
    "    outputs = text_model(**inputs)\n",
    "\n",
    "    # Take the mean across all token embeddings to get a single vector (pooled representation)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Convert to NumPy array and detach from computation graph\n",
    "    return embeddings[0].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7964d-de58-40ae-a053-6d0303cf63d3",
   "metadata": {},
   "source": [
    "Below is the contents that is returned from the vector database after embedding the query and doing a similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc404b10-183d-46f0-a7d4-7f302b55c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectordb_content(user_query: str, action: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves relevant text chunks from the appropriate Qdrant collection\n",
    "    based on the query type and returns a string format.\n",
    "\n",
    "    This function powers the retrieval side of the RAG pipeline\n",
    "    for queries that are classified as either OPENAI-related or 10-K related.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input question.\n",
    "        action (str): The classification label from the router (e.g., \"OPENAI_QUERY\", \"10K_DOCUMENT_QUERY\").\n",
    "\n",
    "    Returns:\n",
    "        str: output from the source\n",
    "    \"\"\"\n",
    "\n",
    "    # Define mapping of routing labels to their respective Qdrant collections\n",
    "    collections = {\n",
    "        \"OPENAI_QUERY\": \"opnai_data\",           # Collection of OpenAI documentation embeddings\n",
    "        \"10K_DOCUMENT_QUERY\": \"10k_data\"        # Collection of 10-K financial document embeddings\n",
    "    }\n",
    " \n",
    "    try:\n",
    "        # Ensure that the provided action is valid\n",
    "        if action not in collections:\n",
    "            return \"Invalid action type for retrieval.\"\n",
    "\n",
    "        # Step 1: Convert the user query into a dense vector (embedding)\n",
    "        try:\n",
    "            query = get_text_embeddings(user_query)\n",
    "        except Exception as embed_err:\n",
    "            return f\"Embedding error: {embed_err}\"  # Fail early if embedding fails\n",
    "\n",
    "        # Step 2: Retrieve top-matching chunks from the relevant Qdrant collection\n",
    "        try:\n",
    "            text_hits = quadrant_client.query_points(\n",
    "                collection_name=collections[action],  # Choose the right collection based on routing\n",
    "                query=query,                          # The embedding of the user's query\n",
    "                limit=3                               # Fetch top 3 relevant chunks\n",
    "            ).points\n",
    "        except Exception as qdrant_err:\n",
    "            return f\"Vector DB query error: {qdrant_err}\"  # Handle Qdrant access issues\n",
    "\n",
    "        # Extract the raw content from the retrieved vector hits\n",
    "        contents = [point.payload['content'] for point in text_hits]\n",
    "\n",
    "        # If no relevant content is found, return early\n",
    "        if not contents:\n",
    "            return \"No relevant content found in the database.\"\n",
    "        return '\\n'.join(contents)\n",
    "            \n",
    "    # Catch any unforeseen errors in the overall process\n",
    "    except Exception as err:\n",
    "        return f\"Unexpected error: {err}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196e99e-c1d1-43d2-8ef8-e74aabcee8b4",
   "metadata": {},
   "source": [
    "Let's create the semantic caching that takes a sourcing function as an argument for the ask function. We will also include some statistics for the caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352e6038-da80-460d-b5c7-cd63a44cf764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticCaching:\n",
    "\n",
    "    def __init__(self, json_file='cache.json', clear_on_init=False):\n",
    "        # Initialize Faiss index with Euclidean distance\n",
    "        self.index = faiss.IndexFlatL2(768)  # Use IndexFlatL2 with Euclidean distance\n",
    "        if self.index.is_trained:\n",
    "            print('Index trained')\n",
    "\n",
    "        # Initialize Sentence Transformer model\n",
    "        self.encoder = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n",
    "\n",
    "        # Euclidean distance threshold for cache hits (lower = more similar)\n",
    "        self.euclidean_threshold = 0.2\n",
    "\n",
    "        # JSON file to persist cache entries\n",
    "        self.json_file = json_file\n",
    "\n",
    "        # Load cache or clear already loaded cache\n",
    "        if clear_on_init:\n",
    "          self.clear_cache()\n",
    "        else:\n",
    "          self.load_cache()\n",
    "\n",
    "        self.stats = {\n",
    "            \"hits\": 0,\n",
    "            \"miss\": 0\n",
    "        }\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"\n",
    "        Clears in-memory cache, resets FAISS index, and overwrites cache.json with an empty structure.\n",
    "        \"\"\"\n",
    "        self.cache = {\n",
    "            'questions': [],\n",
    "            'embeddings': [],\n",
    "            'answers': [],\n",
    "            'response_text': []\n",
    "        }\n",
    "        self.index = faiss.IndexFlatL2(768)  # Reinitialize FAISS index\n",
    "        self.save_cache()\n",
    "        print(\"Semantic cache cleared.\")\n",
    "\n",
    "    def load_cache(self):\n",
    "        \"\"\"Load existing cache or initialize empty structure.\"\"\"\n",
    "        try:\n",
    "            with open(self.json_file, 'r') as file:\n",
    "                self.cache = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "          # Structure: lists of questions, embeddings, answers, and full response text\n",
    "            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Persist cache back to disk.\"\"\"\n",
    "        with open(self.json_file, 'w') as file:\n",
    "            json.dump(self.cache, file)\n",
    "\n",
    "    def ask(self, question: str, source:Callable) -> str:\n",
    "        \"\"\"\n",
    "        Returns a cached answer if within threshold, otherwise generates,\n",
    "        caches, and returns a new answer.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Encode the incoming question\n",
    "            l = [question]\n",
    "            # embedding = self.encoder.encode(l)\n",
    "            embedding = self.encoder.encode(l, normalize_embeddings=True)\n",
    "\n",
    "            # Search for the nearest neighbor in the index\n",
    "            D, I = self.index.search(embedding, 1)\n",
    "\n",
    "            # 3) If a neighbor exists and is within threshold ‚Üí cache hit\n",
    "            if D[0] >= 0:\n",
    "                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
    "                    row_id = int(I[0][0])\n",
    "                    print(f'Cache hit at row: {row_id} with score {1 - D[0][0]}') #score inversed to show similarity\n",
    "                    print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "                    self.stats[\"hits\"]+=1\n",
    "                    return self.cache['response_text'][row_id]\n",
    "\n",
    "            # Handle the case when there are not enough results or Euclidean distance is not met\n",
    "            answer = self.generate_answer(question, source)\n",
    "\n",
    "            # Append new entry to cache\n",
    "            self.cache['questions'].append(question)\n",
    "            self.cache['embeddings'].append(embedding[0].tolist())\n",
    "            self.cache['answers'].append(answer)\n",
    "            self.cache['response_text'].append(answer)\n",
    "            self.index.add(embedding)\n",
    "            self.save_cache()\n",
    "            self.stats[\"miss\"]+=1\n",
    "            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n",
    "\n",
    "    def generate_answer(self, question: str, source: Callable) -> str:\n",
    "        \"\"\"\n",
    "        Always use the Traversaal Ares API for new answers.\n",
    "        Returns (full API result dict, extracted response_text).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = source(question)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")\n",
    "\n",
    "    def analyze_cache_performance(self):\n",
    "        \"\"\"Analyze and visualize cache performance.\"\"\"\n",
    "    \n",
    "        # Get cache statistics\n",
    "        stats = self.stats\n",
    "        \n",
    "        display(Markdown(\"# üìä Cache Performance Analysis\"))\n",
    "        hit_rate = self.stats[\"hits\"]/(self.stats[\"hits\"]+self.stats[\"miss\"]) * 100\n",
    "        estimated_cost_savings =  self.stats[\"hits\"] * 0.002  # Rough estimate: $0.002 per query\n",
    "        time_saved = self.stats[\"hits\"] * 2 #average time of the LLM\n",
    "        # Display statistics\n",
    "        stats_html = f\"\"\"\n",
    "        <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "            <h3>üéØ Overall Performance</h3>\n",
    "            <div style=\"display: flex; justify-content: space-around; flex-wrap: wrap;\">\n",
    "                <div style=\"text-align: center; margin: 10px;\">\n",
    "                    <h2 style=\"color: #007bff; margin: 0;\">{self.stats['miss']}</h2>\n",
    "                    <p><strong>Cache Entries</strong></p>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin: 10px;\">\n",
    "                    <h2 style=\"color: #28a745; margin: 0;\">{hit_rate:.1f}%</h2>\n",
    "                    <p><strong>Hit Rate</strong></p>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin: 10px;\">\n",
    "                    <h2 style=\"color: #ffc107; margin: 0;\">{time_saved:.1f}s</h2>\n",
    "                    <p><strong>Time Saved</strong></p>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin: 10px;\">\n",
    "                    <h2 style=\"color: #17a2b8; margin: 0;\">${estimated_cost_savings:.3f}</h2>\n",
    "                    <p><strong>Est. Cost Saved</strong></p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
    "            <h3>üìà Detailed Statistics</h3>\n",
    "            <p><strong>Total Queries:</strong> {self.stats['hits']+self.stats['miss']}</p>\n",
    "            <p><strong>Cache Hits:</strong> {self.stats['hits']}</p>\n",
    "            <p><strong>Cache Misses:</strong> {self.stats['miss']}</p>\n",
    "            <p><strong>Average Speedup:</strong> ~10x faster for cached responses</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(stats_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b2f420-0543-4e7a-81ee-2713f72933c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic cache cleared.\n"
     ]
    }
   ],
   "source": [
    "cache = SemanticCaching(clear_on_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bd96a-1aee-4a2d-85e8-96ee4ba6d00a",
   "metadata": {},
   "source": [
    "### Internet Query\n",
    "\n",
    "For testing let's see if the cache.ask call really caches the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db85ed05-3e95-40b0-a070-ee86770849ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting your response from the internet üåê ...\n",
      "Time taken: 3.934s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_internet = functools.partial(get_internet_content, action=\"INTERNET_QUERY\")\n",
    "cache.ask(\"What is the capital of France?\", get_internet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5458ede-6168-45fa-984a-b59e4098c0e2",
   "metadata": {},
   "source": [
    "---\n",
    "Let's try again with a question that is semantically equivalent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed57729-1b0d-46d3-9fb4-d32186790936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit at row: 0 with score 0.9677788019180298\n",
      "Time taken: 0.063s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.ask(\"What is France's capital?\", get_internet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16f96d-5aa7-4744-93ce-e4ba8a7e16f4",
   "metadata": {},
   "source": [
    "---\n",
    "Notice the second query has a score > .9 hence it took a cache hit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad6515-1979-4ccb-8404-0ba5220c3698",
   "metadata": {},
   "source": [
    "### Vector Database Query\n",
    "\n",
    "Again we can see similar type of behavior when we have two queries that are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4369ae51-430e-4afe-a737-d538c5d9b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.074s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(6,946)\\n(1,025)\\nProvision for (benefit from) income taxes\\n(192)\\n(492)\\nLoss from equity method investments\\n(34)\\n(37)\\nNet loss including non-controlling interests\\n(6,788)\\n(570)\\nLess: net loss attributable to non-controlling interests, net of tax\\n(20)\\n(74)\\nNet loss attributable to Uber Technologies, In'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vectordb = functools.partial(get_vectordb_content, action=\"10K_DOCUMENT_QUERY\")\n",
    "out = cache.ask(\"What was Uber revenue in 2021?\", get_vectordb)\n",
    "out[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e79a2cf1-f7e0-4580-96dc-2f15e39532e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit at row: 1 with score 0.931459367275238\n",
      "Time taken: 0.027s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(6,946)\\n(1,025)\\nProvision for (benefit from) income taxes\\n(192)\\n(492)\\nLoss from equity method investments\\n(34)\\n(37)\\nNet loss including non-controlling interests\\n(6,788)\\n(570)\\nLess: net loss attributable to non-controlling interests, net of tax\\n(20)\\n(74)\\nNet loss attributable to Uber Technologies, In'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = cache.ask(\"How much was Uber's 2021 revenue?\", get_vectordb)\n",
    "out[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9e487-6264-4bd9-b5e9-f804a189882f",
   "metadata": {},
   "source": [
    "## 5. LLM Response\n",
    "\n",
    "The cache.ask function can work for any part of the pipeline including the source functions but we want to cache the LLM response not the sources so let's create a rag_response function. First let create the routing and sub-query division of the query. We want routing so a decision is made whether to choose the the vector database's 10-k filing, vector database's OpenAI technology , or the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b515f9c7-f64e-4dac-965e-27dd82d4dcf0",
   "metadata": {},
   "source": [
    "### Router & Sub-Query Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6c408a-241c-4582-abfd-9cccf5f1cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the OpenAI client with the retrieved API key\n",
    "# This client will be used for:\n",
    "# - Query classification via the router prompt\n",
    "# - Potentially generating responses from retrieved context\n",
    "openaiclient = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def router(user_query:str) -> str:\n",
    "    router_system_prompt =f\"\"\"\n",
    "    As a professional query router, your objective is to correctly classify user input into one of three categories based on the source most relevant for answering the query:\n",
    "    1. \"OPENAI_QUERY\": If the user's query appears to be answerable using information from OpenAI's official documentation, tools, models, APIs, or services (e.g., GPT, ChatGPT, embeddings, moderation API, usage guidelines).\n",
    "    2. \"10K_DOCUMENT_QUERY\": If the user's query pertains to a collection of documents from the 10k annual reports, datasets, or other structured documents, typically for research, analysis, or financial content.\n",
    "    3. \"INTERNET_QUERY\": If the query is neither related to OpenAI nor the 10k documents specifically, or if the information might require a broader search (e.g., news, trends, tools outside these platforms), route it here.\n",
    "\n",
    "    Your decision should be made by assessing the domain of the query.\n",
    "\n",
    "    Always respond in this valid JSON format:\n",
    "    {{\n",
    "        \"action\": \"OPENAI_QUERY\" or \"10K_DOCUMENT_QUERY\" or \"INTERNET_QUERY\",\n",
    "        \"reason\": \"brief justification\",\n",
    "        \"answer\": \"AT MAX 5 words answer. Leave empty if INTERNET_QUERY\"\n",
    "    }}\n",
    "\n",
    "    EXAMPLES:\n",
    "\n",
    "    - User: \"How to fine-tune GPT-3?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"OPENAI_QUERY\",\n",
    "        \"reason\": \"Fine-tuning is OpenAI-specific\",\n",
    "        \"answer\": \"Use fine-tuning API\"\n",
    "    }}\n",
    "\n",
    "    - User: \"Where can I find the latest financial reports for the last 10 years?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"10K_DOCUMENT_QUERY\",\n",
    "        \"reason\": \"Query related to annual reports\",\n",
    "        \"answer\": \"Access through document database\"\n",
    "    }}\n",
    "\n",
    "    - User: \"Top leadership styles in 2024\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"INTERNET_QUERY\",\n",
    "        \"reason\": \"Needs current leadership trends\",\n",
    "        \"answer\": \"\"\n",
    "    }}\n",
    "\n",
    "    - User: \"What's the difference between ChatGPT and Claude?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"INTERNET_QUERY\",\n",
    "        \"reason\": \"Cross-comparison of different providers\",\n",
    "        \"answer\": \"\"\n",
    "    }}\n",
    "\n",
    "    Strictly follow this format for every query, and never deviate.\n",
    "    User: {user_query}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Query the GPT-4 model with the router prompt and user input\n",
    "        response = openaiclient.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[{\"role\": \"system\", \"content\": router_system_prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract and parse the model's JSON response\n",
    "        task_response = response.choices[0].message.content\n",
    "        json_match = re.search(r\"\\{.*\\}\", task_response, re.DOTALL)\n",
    "        json_text = json_match.group()\n",
    "        parsed_response = json.loads(json_text)\n",
    "        return parsed_response\n",
    "\n",
    "    # Handle OpenAI API errors (e.g., rate limits, authentication)\n",
    "    except OpenAIError as api_err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"OpenAI API error: {api_err}\",\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "\n",
    "    # Handle case where model response isn't valid JSON\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"JSON parsing error: {json_err}\",\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "\n",
    "    # Catch-all for any other unforeseen issues\n",
    "    except Exception as err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"Unexpected error: {err}\",\n",
    "            \"answer\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c4ebd-31f0-4574-8be3-8227b7db07ae",
   "metadata": {},
   "source": [
    "Let's do a sub-query division where we take any complex query and break it down to simple or atomic queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b74e8cd-0eb5-4277-ba6c-3e2a3365bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_queries(user_query):\n",
    "    sub_queries_prompt = f\"\"\"\n",
    "    You are a query router. If the input contains multiple distinct questions, break it into sub-questions making sure each question has enough context. Otherwise, keep it as one. Return a JSON object like:\n",
    "\n",
    "      Rules:\n",
    "      - If a time frame, location, or other modifier applies to the whole compound sentence, propagate it to *each* sub-question.\n",
    "      - Ensure each sub-question is fully self-contained and unambiguous.\n",
    "      - Use the same tense and wording as the original query, unless clarification is required for completeness.\n",
    "\n",
    "    {{\n",
    "        \"subQuestions\": [\"...\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    {sub_queries_prompt}\n",
    "\n",
    "    Query: \"{user_query}\"\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    response = openaiclient.chat.completions.create(\n",
    "          model=\"gpt-4.1\",\n",
    "          messages=[\n",
    "              {\"role\": \"system\", \"content\": system_prompt},\n",
    "          ]\n",
    "      )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b38bba-64a1-49f8-94c0-3047b71bb909",
   "metadata": {},
   "source": [
    "### RAG Components\n",
    "\n",
    "We will incorporate different functions to complete the rag system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93727979-c2d2-4287-935b-b4abac1d283c",
   "metadata": {},
   "source": [
    "The function rag_formatted_response returns the LLM response with the context retrieved from the vectordb or internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a20ab25c-1d96-4708-9248-1c4052cb6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_formatted_response(user_query: str, context: str):\n",
    "    \"\"\"\n",
    "    Generate a response to the user query using the provided context,\n",
    "    with article references formatted as [1][2], etc from vector db or response from a internet search\n",
    "\n",
    "    This function performs the final step in the RAG pipeline‚Äîsynthesizing an answer\n",
    "    from retrieved document chunks (context). It prompts the model to generate a\n",
    "    grounded response, explicitly citing sources using a reference format.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's original question.\n",
    "        context (str): List of text chunks retrieved from Qdrant (10-K or OpenAI docs) in str format or from a internet search.\n",
    "\n",
    "    Returns:\n",
    "        str: A generated response grounded in the retrieved context, with numbered citations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct a RAG prompt that includes both:\n",
    "    # 1. The user's query\n",
    "    # 2. The supporting context documents\n",
    "    # The prompt instructs the model to answer using only the provided context,\n",
    "    # and to include citations like [1], [2], etc. based on chunk IDs or order.\n",
    "    rag_prompt = f\"\"\"\n",
    "       Based on the given context, answer the user query: {user_query}\\nContext:\\n{context}\n",
    "       and employ references to the ID of articles provided [ID], ensuring their relevance to the query.\n",
    "       The referencing should always be in the format of [1][2]... etc. </instructions>\n",
    "    \"\"\"\n",
    "\n",
    "    #  Call GPT-4o to generate the response using the RAG-style prompt\n",
    "    response = openaiclient.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": rag_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return the model's generated answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d264b5d-3601-48d6-92c5-a30e7fa500c9",
   "metadata": {},
   "source": [
    "The retrieve function chooses which source whether it is the db uses the above function the get a response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4327097-d4fc-4532-becd-d660428a0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_response(user_query: str, action: str):\n",
    "    \"\"\"\n",
    "    Retrieves relevant text chunks from the appropriate Qdrant collection\n",
    "    based on the query type, then generates a response using RAG.\n",
    "\n",
    "    This function powers the retrieval and response generation pipeline\n",
    "    for queries that are classified as either OPENAI-related or 10-K related.\n",
    "    It uses semantic search to fetch relevant context from a Qdrant vector store\n",
    "    and then generates a response using that context via a RAG prompt.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input question.\n",
    "        action (str): The classification label from the router (e.g., \"OPENAI_QUERY\", \"10K_DOCUMENT_QUERY\").\n",
    "\n",
    "    Returns:\n",
    "        str: A model-generated response grounded in retrieved documents, or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define mapping of routing labels to their respective Qdrant collections\n",
    "    collections = {\n",
    "        \"OPENAI_QUERY\": \"opnai_data\",           # Collection of OpenAI documentation embeddings\n",
    "        \"10K_DOCUMENT_QUERY\": \"10k_data\"        # Collection of 10-K financial document embeddings\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Ensure that the provided action is valid\n",
    "        if action not in collections:\n",
    "            return \"Invalid action type for retrieval.\"\n",
    "\n",
    "        # Step 1: Convert the user query into a dense vector (embedding)\n",
    "        try:\n",
    "            query = get_text_embeddings(user_query)\n",
    "        except Exception as embed_err:\n",
    "            return f\"Embedding error: {embed_err}\"  # Fail early if embedding fails\n",
    "\n",
    "        # Step 2: Retrieve top-matching chunks from the relevant Qdrant collection\n",
    "        try:\n",
    "            text_hits = quadrant_client.query_points(\n",
    "                collection_name=collections[action],  # Choose the right collection based on routing\n",
    "                query=query,                          # The embedding of the user's query\n",
    "                limit=3                               # Fetch top 3 relevant chunks\n",
    "            ).points\n",
    "        except Exception as qdrant_err:\n",
    "            return f\"Vector DB query error: {qdrant_err}\"  # Handle Qdrant access issues\n",
    "\n",
    "        # Extract the raw content from the retrieved vector hits\n",
    "        contents = [point.payload['content'] for point in text_hits]\n",
    "\n",
    "        # If no relevant content is found, return early\n",
    "        if not contents:\n",
    "            return \"No relevant content found in the database.\"\n",
    "\n",
    "        # Step 3: Pass the retrieved context to the RAG model to generate a response\n",
    "        try: # attempt to see if cache data exists otherwise generate response and store in cache\n",
    "            response = rag_formatted_response(user_query, contents)\n",
    "            return response\n",
    "        except Exception as rag_err:\n",
    "            return f\"RAG response error: {rag_err}\"  # Handle generation failures\n",
    "\n",
    "    # Catch any unforeseen errors in the overall process\n",
    "    except Exception as err:\n",
    "        return f\"Unexpected error: {err}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f795f12-1112-4678-afbf-b92904cf2d32",
   "metadata": {},
   "source": [
    "And finally the agentic_semantic_rag which returns the final results either through cached results or from the LLM with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "199f003a-9fcf-46f5-9ff1-ef658590f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "routes = {\n",
    "    \"OPENAI_QUERY\": retrieve_and_response,\n",
    "    \"10K_DOCUMENT_QUERY\": retrieve_and_response,\n",
    "    \"INTERNET_QUERY\": get_internet_content,\n",
    "}\n",
    "\n",
    "def agentic_semantic_rag(user_query: str):\n",
    "    \"\"\"\n",
    "    Main function that runs the full Agentic RAG system with semantic cache and sub-query divsion.\n",
    "\n",
    "    This function takes a user's question, decides what type of query it is (OpenAI-related,\n",
    "    financial document-related, or general internet), and then calls the right function\n",
    "    to handle it. Finally, it prints out the full conversation and response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input question.\n",
    "\n",
    "    Returns:\n",
    "        None (It just prints the result nicely to the console)\n",
    "    \"\"\"\n",
    "\n",
    "    #  Terminal color codes to make the printed output easier to read and visually structured\n",
    "    CYAN = \"\\033[96m\"\n",
    "    GREY = \"\\033[90m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Print the user's original question to the console\n",
    "        print(f\"{BOLD}{CYAN}üë§ User Query:{BOLD}{GREY} {user_query}\\n\")\n",
    "\n",
    "        subQuestions = json.loads(sub_queries(user_query))['subQuestions']\n",
    "        # Step 2: Iterrate over sub-query division and use the router (powered by GPT) to decide which route the query belongs to\n",
    "        output = []\n",
    "        for subQuestion in subQuestions:\n",
    "            \n",
    "            try:\n",
    "                response = router(subQuestion)\n",
    "            except Exception as route_err:\n",
    "                # If something goes wrong while classifying the query, show an error message\n",
    "                print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "                print(f\"Routing error: {route_err}\\n\")\n",
    "                return\n",
    "    \n",
    "            # Extract the routing decision and the reason behind it\n",
    "            action = response.get(\"action\")  # e.g., \"OPENAI_QUERY\"\n",
    "            reason = response.get(\"reason\")  # e.g., \"Related to OpenAI tools\"\n",
    "    \n",
    "            # Step 3: Show the selected route and why it was chosen\n",
    "            print(f\"{GREY}üìç Selected Route: {action}\")\n",
    "            print(f\"üìù Reason: {reason}\")\n",
    "            print(f\"‚öôÔ∏è Processing query...{RESET}\\n\")\n",
    "    \n",
    "            # Step 4: Call the correct function depending on the route (retrieval or web search)\n",
    "            try:\n",
    "                route_function = routes.get(action)  # Find the function to use for this route\n",
    "                if route_function:\n",
    "                    rf = functools.partial(route_function, action=action)\n",
    "                    result = cache.ask(subQuestion, rf)\n",
    "                    output.append(result)\n",
    "                else:\n",
    "                    result = f\"Unsupported action: {action}\"  # Catch unknown routing types\n",
    "            except Exception as exec_err:\n",
    "                result = f\"Execution error: {exec_err}\"  # Handle failure in the chosen route function\n",
    "    \n",
    "            # Step 5: Print the final response to the user\n",
    "            \n",
    "        result = '\\n'.join(output)\n",
    "        print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "        print(f\"{result}\\n\")\n",
    "        return result\n",
    "\n",
    "    except Exception as err:\n",
    "        # Catch-all for any unexpected errors in the overall logic\n",
    "        print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "        print(f\"Unexpected error occurred: {err}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b7448d3-b9c7-4c3c-8ae2-0dc6038eb55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic cache cleared.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (type 'quit' to exit):  What were Uber and Lyft‚Äôs revenues in 2021?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[1m\u001b[90m What were Uber and Lyft‚Äôs revenues in 2021?\n",
      "\n",
      "\u001b[90müìç Selected Route: 10K_DOCUMENT_QUERY\n",
      "üìù Reason: Question about company's financials\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Time taken: 1.495s\n",
      "\u001b[90müìç Selected Route: 10K_DOCUMENT_QUERY\n",
      "üìù Reason: Revenue figure from annual report\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Time taken: 1.235s\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Uber's revenue in 2021 was $17,455 million [2].\n",
      "The context does not provide information on Lyft's revenue in 2021.\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (type 'quit' to exit):  Uber's 2021 revenue?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[1m\u001b[90m Uber's 2021 revenue?\n",
      "\n",
      "\u001b[90müìç Selected Route: 10K_DOCUMENT_QUERY\n",
      "üìù Reason: Request for historical financial data\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Cache hit at row: 0 with score 1.0\n",
      "Time taken: 0.065s\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Uber's revenue in 2021 was $17,455 million [2].\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (type 'quit' to exit):  Give me revenue numbers by Uber in 2021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[1m\u001b[90m Give me revenue numbers by Uber in 2021\n",
      "\n",
      "\u001b[90müìç Selected Route: 10K_DOCUMENT_QUERY\n",
      "üìù Reason: Uber's revenue is found in 10k reports\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Cache hit at row: 0 with score 1.0\n",
      "Time taken: 0.162s\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Uber's revenue in 2021 was $17,455 million [2].\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (type 'quit' to exit):  How do I authenticate with OpenAI's API?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[1m\u001b[90m How do I authenticate with OpenAI's API?\n",
      "\n",
      "\u001b[90müìç Selected Route: OPENAI_QUERY\n",
      "üìù Reason: OpenAI API authentication is specific\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Time taken: 8.015s\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "To authenticate with OpenAI's API, you will need to use API keys for authentication [1]. The process is as follows:\n",
      "\n",
      "- First, create or manage your API key in your organization settings [1].\n",
      "- Remember not to share your API key or expose it in client-side code [1].\n",
      "- API keys should be securely loaded from an environment variable or key management service on the server [1].\n",
      "- Provide your API keys via HTTP Bearer authentication [1].\n",
      "  \n",
      "You can pass a header to specify which organization and project to use for an API request, especially if you belong to multiple organizations or access projects through a legacy user API key [1].\n",
      "\n",
      "Here is an example of the curl command for authorization:\n",
      "```\n",
      "curl https://api.openai.com/v1/models \\\n",
      "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
      "  -H \"OpenAI-Organization: YOUR_ORG_ID\" \\\n",
      "  -H \"OpenAI-Project: $PROJECT_ID\"\n",
      "```\n",
      "\n",
      "In this command, replace `$OPENAI_API_KEY` with your API key, `YOUR_ORG_ID` with your ID, and `$PROJECT_ID` with your project ID [1].\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (type 'quit' to exit):  How do text embeddings work?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[1m\u001b[90m How do text embeddings work?\n",
      "\n",
      "\u001b[90müìç Selected Route: OPENAI_QUERY\n",
      "üìù Reason: Embeddings are part of OpenAI API\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Time taken: 6.270s\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Text embeddings work by converting pieces of text data into vector form that preserves aspects of their content or meaning. This is done through processing the text in chunks known as tokens, which represent common sequences of characters[1]. Text that has similar content or meaning will have closer embeddings than those that are unrelated[2]. OpenAI offers text embedding models that accept a text string as input and produce an embedding vector as output. These embeddings serve useful for tasks like search, clustering, recommendation systems, anomaly detection, and classification among others[2]. For example, you can request embeddings through OpenAI's API by providing the text you want to input, specifying the model, and the encoding format you want the embeddings returned in[3]. The result would be an embedding vector, which is a list of floats. The length of this vector depends on the selected model[4].\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (type 'quit' to exit):  Explain text embeddings to me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[1m\u001b[90m Explain text embeddings to me\n",
      "\n",
      "\u001b[90müìç Selected Route: OPENAI_QUERY\n",
      "üìù Reason: Text embeddings are OpenAI feature\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Cache hit at row: 3 with score 0.8538163304328918\n",
      "Time taken: 0.082s\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Text embeddings work by converting pieces of text data into vector form that preserves aspects of their content or meaning. This is done through processing the text in chunks known as tokens, which represent common sequences of characters[1]. Text that has similar content or meaning will have closer embeddings than those that are unrelated[2]. OpenAI offers text embedding models that accept a text string as input and produce an embedding vector as output. These embeddings serve useful for tasks like search, clustering, recommendation systems, anomaly detection, and classification among others[2]. For example, you can request embeddings through OpenAI's API by providing the text you want to input, specifying the model, and the encoding format you want the embeddings returned in[3]. The result would be an embedding vector, which is a list of floats. The length of this vector depends on the selected model[4].\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (type 'quit' to exit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Search Ended!\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"  # Initialize user_input to an empty string\n",
    "cache.clear_cache() #clear the cache just in case\n",
    "while user_input != \"quit\":  # Loop continues until user types \"quit\"\n",
    "    user_input = input(\"Enter something (type 'quit' to exit): \")\n",
    "    if user_input != \"quit\":\n",
    "        #print(f\"You entered: {user_input}\")\n",
    "        response = agentic_semantic_rag(user_input)\n",
    "        print(\"-\" * 70) \n",
    "    \n",
    "\n",
    "print(\"Query Search Ended!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70447d9-0637-4502-8b06-8baa194456d3",
   "metadata": {},
   "source": [
    "### STATS!\n",
    "\n",
    "Take a look at some caching statistics. The costs and time related related information are just very rough estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9afc3c43-f581-469f-9da4-68ec15474e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# üìä Cache Performance Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
       "            <h3>üéØ Overall Performance</h3>\n",
       "            <div style=\"display: flex; justify-content: space-around; flex-wrap: wrap;\">\n",
       "                <div style=\"text-align: center; margin: 10px;\">\n",
       "                    <h2 style=\"color: #007bff; margin: 0;\">6</h2>\n",
       "                    <p><strong>Cache Entries</strong></p>\n",
       "                </div>\n",
       "                <div style=\"text-align: center; margin: 10px;\">\n",
       "                    <h2 style=\"color: #28a745; margin: 0;\">45.5%</h2>\n",
       "                    <p><strong>Hit Rate</strong></p>\n",
       "                </div>\n",
       "                <div style=\"text-align: center; margin: 10px;\">\n",
       "                    <h2 style=\"color: #ffc107; margin: 0;\">10.0s</h2>\n",
       "                    <p><strong>Time Saved</strong></p>\n",
       "                </div>\n",
       "                <div style=\"text-align: center; margin: 10px;\">\n",
       "                    <h2 style=\"color: #17a2b8; margin: 0;\">$0.010</h2>\n",
       "                    <p><strong>Est. Cost Saved</strong></p>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
       "            <h3>üìà Detailed Statistics</h3>\n",
       "            <p><strong>Total Queries:</strong> 11</p>\n",
       "            <p><strong>Cache Hits:</strong> 5</p>\n",
       "            <p><strong>Cache Misses:</strong> 6</p>\n",
       "            <p><strong>Average Speedup:</strong> ~10x faster for cached responses</p>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cache.analyze_cache_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924052d9-342e-4a14-b243-605b16f3a033",
   "metadata": {},
   "source": [
    "# 6. üìñ Additional Resources\n",
    "\n",
    "### üìö Further Reading\n",
    "\n",
    "- [RAG Papers and Research](https://arxiv.org/abs/2005.11401) - Original RAG paper\n",
    "- [Vector Database Comparison](https://liquidmetal.ai/casesAndBlogs/vector-comparison/) \n",
    "- [Semantic Search Best Practices](https://www.pinecone.io/learn/semantic-search/)\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/retrievers/)\n",
    "\n",
    "### üõ†Ô∏è Tools and Frameworks\n",
    "\n",
    "- **Vector Databases**: Qdrant, Pinecone, Weaviate, Chroma\n",
    "- **Embedding Models**: OpenAI, Cohere, Sentence-Transformers\n",
    "- **RAG Frameworks**: LangChain, LlamaIndex, Haystack\n",
    "- **Monitoring**: Weights & Biases, MLflow, Arize\n",
    "\n",
    "### üéì Next Learning Steps\n",
    "\n",
    "1. **Advanced RAG**: Multi-hop reasoning, graph-based retrieval\n",
    "2. **Fine-tuning**: Custom embedding models for domain data\n",
    "3. **Evaluation**: RAGAS, answer quality metrics\n",
    "4. **Production**: Deployment, scaling, monitoring\n",
    "\n",
    "---\n",
    "\n",
    "*Thank you for following along with this comprehensive semantic search engine tutorial! üöÄ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f6919-b7f3-4df2-9293-35e7322143ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
